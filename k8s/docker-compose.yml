version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./ollama:/root/.ollama
    command: serve
    
  ollama-models:
    image: ollama/ollama:latest
    depends_on:
      - ollama
    volumes:
      - ./ollama:/root/.ollama
    entrypoint: >
      sh -c "
      sleep 5 &&
      ollama pull llama3.2:3b &&
      echo 'Models loaded'"
    
  booking-agent:
    build: ./backend
    ports:
      - "8000:8000"
    depends_on:
      - ollama
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3.2:3b
    volumes:
      - ./backend:/app
    command: python app.py
    
  mock-api:
    build: ./Restaurant-Booking-Mock-API-Server
    ports:
      - "8547:8547"
    command: python -m app